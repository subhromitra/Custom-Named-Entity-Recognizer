{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1> Custom Named Entity Recognizer</h1>\n",
    "\n",
    "Named entity recognizer's are generally trained on texts from web articles or news articles. In such cases they are not able to recognize entities from finance or medical domain. For such use cases, we need to train the NER using domain specific labelled data. \n",
    "\n",
    "If training data is large then we may train an NER from scratch. But if we don't have sufficient data then we may retrain a previously trained NER with new data.\n",
    "\n",
    "**For this use case since our data is small we would be retraining a pre-trained model.**\n",
    "<br>\n",
    "\n",
    "<h3>Problem Description</h3>\n",
    "\n",
    "The core problem that we are trying to solve here is :- Given a text (sentence), we have to find out the different types of entities in that text, e.g - names of people, organizations, places etc. \n",
    "\n",
    "We can model it as a **many-to-many** sequence prediction problem, i.e, input is a sentence (set of tokens) & output is a set of tags describing each token.\n",
    "<br>\n",
    "\n",
    "<h3>Dataset Description :-</h3>\n",
    "\n",
    "The dataset used is BC5CDR-disease from NCBI. One can find the dataset here - https://github.com/dmis-lab/biobert . We have nearly 2K labelled sentences.\n",
    "<br>\n",
    "\n",
    "<h3>Plan of Attack</h3>\n",
    "\n",
    "We are going to retrain a pre-trained model. The model we are going to use is a **1D-CNN model** provided by SpaCy. SpaCy's NLP pipeline provides a tagger, a parser & a named entity recognizer. We are going to disable the tagger & parser before starting to retrain, since our objective is to train the NER only."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "AKViPFaUtQ42"
   },
   "outputs": [],
   "source": [
    "import spacy\n",
    "from spacy import displacy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "CTcKvKH1u1qU",
    "outputId": "dd9d07b4-3334-45fd-b503-697f07618265"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## For training models using GPU. Use spacy.prefer_gpu() if you are not sure whether GPU is available or not.\n",
    "## require_gpu() will return False if GPU is unavailable. Always execute below line before loading Spacy language models\n",
    "\n",
    "# spacy.require_gpu()\n",
    "spacy.prefer_gpu()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "tlTJmWAgt2cz",
    "outputId": "dba8a9d4-b27d-4701-f62b-ef4f0bddb9e9"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['tagger', 'parser', 'ner']"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "nlp.pipe_names"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "UGCIFotuaYCD"
   },
   "source": [
    "## Formatting data for training :-\n",
    "\n",
    "Format of data to be supplied to SpaCy model - \n",
    "\n",
    "[[\"Sent_1\",{'entities':[(start, end, label),(start, end, label),....]}],\n",
    "\n",
    "[\"Sent_2\",{'entities':[(start, end, label),(start, end, label),....]}], ....]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "oYgAC_YKylh3"
   },
   "outputs": [],
   "source": [
    "def load_data(file_path):\n",
    "    \"\"\"Function to help us prepare data for training\"\"\"\n",
    "    \n",
    "    train_data, entities, sentence, unique_labels = [], [], [], []\n",
    "    current_annot = None\n",
    "    start, end = 0, 0\n",
    "\n",
    "    with open(file_path, 'r') as file:\n",
    "        for line in file:\n",
    "            line = line.strip(\"\\n\").split(\"\\t\")\n",
    "\n",
    "            if len(line) > 1:\n",
    "                label = line[1]\n",
    "                if label != 'O': label = label + \"_Dis\"\n",
    "                word = line[0]\n",
    "                sentence.append(word)\n",
    "                start = end\n",
    "                end += len(word) + 1\n",
    "\n",
    "                if label == \"I_Dis\":\n",
    "                    entities.append((start, end-1, label))\n",
    "                elif label == \"B_Dis\":\n",
    "                    entities.append((start, end-1, label))\n",
    "\n",
    "                if label != 'O' and label not in unique_labels:\n",
    "                    unique_labels.append(label)\n",
    "\n",
    "            elif len(line) == 1:\n",
    "                if len(entities) > 0:\n",
    "                    sentence = \" \".join(sentence)\n",
    "                    train_data.append([sentence,\n",
    "                                       {'entities':entities}])\n",
    "                    start, end = 0, 0\n",
    "                    entities, sentence = [], []\n",
    "\n",
    "    return train_data, unique_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "4V_jLiJc69R5",
    "outputId": "14e39f7e-f109-4956-8921-62218bfd4d4d"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train data size :-  2658\n",
      "Test data size :-  2842\n"
     ]
    }
   ],
   "source": [
    "train_data, unique_labels = load_data(r\"/content/train.tsv\")\n",
    "test_data, _ = load_data(r\"/content/test.tsv\")\n",
    "\n",
    "print(\"Train data size :- \",len(train_data))\n",
    "print(\"Test data size :- \",len(test_data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "sg_h7Fa57gQp",
    "outputId": "e57da67f-571f-4a13-fc71-ddb2d5cbc7fa"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[\"Selegiline - induced postural hypotension in Parkinson ' s disease : a longitudinal study on the effects of drug withdrawal .\", {'entities': [(21, 29, 'B_Dis'), (30, 41, 'I_Dis'), (45, 54, 'B_Dis'), (55, 56, 'I_Dis'), (57, 58, 'I_Dis'), (59, 66, 'I_Dis')]}]\n",
      "[\"OBJECTIVES : The United Kingdom Parkinson ' s Disease Research Group ( UKPDRG ) trial found an increased mortality in patients with Parkinson ' s disease ( PD ) randomized to receive 10 mg selegiline per day and L - dopa compared with those taking L - dopa alone .\", {'entities': [(32, 41, 'B_Dis'), (42, 43, 'I_Dis'), (44, 45, 'I_Dis'), (46, 53, 'I_Dis'), (132, 141, 'B_Dis'), (142, 143, 'I_Dis'), (144, 145, 'I_Dis'), (146, 153, 'I_Dis'), (156, 158, 'B_Dis')]}]\n",
      "['Recently , we found that therapy with selegiline and L - dopa was associated with selective systolic orthostatic hypotension which was abolished by withdrawal of selegiline .', {'entities': [(92, 100, 'B_Dis'), (101, 112, 'I_Dis'), (113, 124, 'I_Dis')]}]\n",
      "['This unwanted effect on postural blood pressure was not the result of underlying autonomic failure . The aims of this study were to confirm our previous findings in a separate cohort of patients and to determine the time course of the cardiovascular consequences of stopping selegiline in the expectation that this might shed light on the mechanisms by which the drug causes orthostatic hypotension .', {'entities': [(375, 386, 'B_Dis'), (387, 398, 'I_Dis')]}]\n",
      "['METHODS : The cardiovascular responses to standing and head - up tilt were studied repeatedly in PD patients receiving selegiline and as the drug was withdrawn .', {'entities': [(97, 99, 'B_Dis')]}]\n"
     ]
    }
   ],
   "source": [
    "for data in train_data[0:5]:\n",
    "  print(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 124
    },
    "id": "cwUppeKaNSob",
    "outputId": "7bfe9d83-a9d1-4a00-8fa4-515eb439155c"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Entities []\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/lib/python3.7/runpy.py:193: UserWarning: [W006] No entities to visualize found in Doc object. If this is surprising to you, make sure the Doc was processed using a model that supports named entity recognition, and check the `doc.ents` property manually if necessary.\n",
      "  \"__main__\", mod_spec)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<span class=\"tex2jax_ignore\"><div class=\"entities\" style=\"line-height: 2.5; direction: ltr\">Torsade de pointes ventricular tachycardia during low dose intermittent dobutamine treatment in a patient with dilated cardiomyopathy and congestive heart failure .</div></span>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "## Checking if SpaCy's NER is able to detect any named entity from clinical data.\n",
    "doc = nlp(test_data[0][0])\n",
    "print(\"Entities\", [(ent.text, ent.label_) for ent in doc.ents])\n",
    "displacy.render(doc, jupyter=True, style = \"ent\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "CO-uqyqtNmGB"
   },
   "source": [
    "**SpaCy's NER model didn't find any entity which was somewhat obvious. It wasn't trained using clinical text data.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "uNmvKAbVbOgK"
   },
   "source": [
    "## Retraining model :-\n",
    "\n",
    "Steps to be followed :-\n",
    "\n",
    "- Add new labels to NER part of pipeline\n",
    "- Disable other stuff in pipeline before training\n",
    "- Start training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "id": "y2DGE1bG7qQI"
   },
   "outputs": [],
   "source": [
    "ner = nlp.get_pipe(\"ner\")\n",
    "\n",
    "for label in unique_labels:\n",
    "    ner.add_label(label)\n",
    "\n",
    "## Disable other stuff in pipeline to only train NER\n",
    "disable_pipes = [pipe for pipe in nlp.pipe_names if pipe != 'ner']    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "id": "U4jgG2MEAyZc"
   },
   "outputs": [],
   "source": [
    "import time\n",
    "import random\n",
    "from spacy.util import minibatch, compounding\n",
    "from pathlib import Path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "mcduZ4FkBNXQ",
    "outputId": "be56a366-5f76-4d63-bbbb-72a6bede918b"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Losses =  {'ner': 881.079833984375}\n",
      "Losses =  {'ner': 2082.011489868164}\n",
      "Losses =  {'ner': 3284.989227294922}\n",
      "Losses =  {'ner': 5649.9578857421875}\n",
      "Losses =  {'ner': 8555.014343261719}\n",
      "Losses =  {'ner': 10774.422836303711}\n",
      "Losses =  {'ner': 12778.362442016602}\n",
      "Losses =  {'ner': 15236.054641723633}\n",
      "Losses =  {'ner': 17730.669326782227}\n",
      "Losses =  {'ner': 20238.814407348633}\n",
      "Losses =  {'ner': 22412.823440551758}\n",
      "Losses =  {'ner': 24920.975311279297}\n",
      "Losses =  {'ner': 26905.799285888672}\n",
      "Losses =  {'ner': 28618.208953857422}\n",
      "Losses =  {'ner': 30844.873428344727}\n",
      "Losses =  {'ner': 32618.490478515625}\n",
      "Losses =  {'ner': 34438.96694946289}\n",
      "Losses =  {'ner': 36409.355545043945}\n",
      "Losses =  {'ner': 38423.28039550781}\n",
      "Losses =  {'ner': 40357.236404418945}\n",
      "Losses =  {'ner': 42049.9609375}\n",
      "Losses =  {'ner': 43933.59896850586}\n",
      "Losses =  {'ner': 46078.80177307129}\n",
      "Losses =  {'ner': 48117.87957763672}\n",
      "Losses =  {'ner': 50354.362213134766}\n",
      "Losses =  {'ner': 51898.64566040039}\n",
      "Losses =  {'ner': 53733.63136291504}\n",
      "Losses =  {'ner': 55561.68421936035}\n",
      "Losses =  {'ner': 58039.321380615234}\n",
      "Losses =  {'ner': 60170.37170410156}\n",
      "Losses =  {'ner': 62090.5205078125}\n",
      "Losses =  {'ner': 64383.16003417969}\n",
      "Losses =  {'ner': 66915.46563720703}\n",
      "Losses =  {'ner': 68880.52258300781}\n",
      "Losses =  {'ner': 70769.51963806152}\n",
      "Losses =  {'ner': 72769.25462341309}\n",
      "Losses =  {'ner': 74582.10766601562}\n",
      "Losses =  {'ner': 76889.88554382324}\n",
      "Losses =  {'ner': 79062.02473449707}\n",
      "Losses =  {'ner': 81144.59338378906}\n",
      "Losses =  {'ner': 83018.38374328613}\n",
      "Losses =  {'ner': 84829.35429382324}\n",
      "Losses =  {'ner': 86496.94940185547}\n",
      "Losses =  {'ner': 87397.9461517334}\n",
      "Losses =  {'ner': 422.2578926086426}\n",
      "Losses =  {'ner': 1195.1566429138184}\n",
      "Losses =  {'ner': 2456.232479095459}\n",
      "Losses =  {'ner': 4312.943416595459}\n",
      "Losses =  {'ner': 5790.232173919678}\n",
      "Losses =  {'ner': 8032.485805511475}\n",
      "Losses =  {'ner': 9547.86429977417}\n",
      "Losses =  {'ner': 11244.983318328857}\n",
      "Losses =  {'ner': 12948.580165863037}\n",
      "Losses =  {'ner': 14525.80814743042}\n",
      "Losses =  {'ner': 16044.326580047607}\n",
      "Losses =  {'ner': 17977.72549057007}\n",
      "Losses =  {'ner': 19717.86167526245}\n",
      "Losses =  {'ner': 21233.94813156128}\n",
      "Losses =  {'ner': 23580.14896774292}\n",
      "Losses =  {'ner': 25469.9744682312}\n",
      "Losses =  {'ner': 27462.382274627686}\n",
      "Losses =  {'ner': 29070.323894500732}\n",
      "Losses =  {'ner': 31074.33320236206}\n",
      "Losses =  {'ner': 32477.304668426514}\n",
      "Losses =  {'ner': 33993.91065597534}\n",
      "Losses =  {'ner': 35353.189739227295}\n",
      "Losses =  {'ner': 37106.04566574097}\n",
      "Losses =  {'ner': 39125.52396774292}\n",
      "Losses =  {'ner': 40627.265575408936}\n",
      "Losses =  {'ner': 42728.345195770264}\n",
      "Losses =  {'ner': 44726.06144332886}\n",
      "Losses =  {'ner': 46564.26203536987}\n",
      "Losses =  {'ner': 48215.34475326538}\n",
      "Losses =  {'ner': 49729.9296836853}\n",
      "Losses =  {'ner': 51459.19518661499}\n",
      "Losses =  {'ner': 53406.78925704956}\n",
      "Losses =  {'ner': 55350.26919174194}\n",
      "Losses =  {'ner': 57079.949031829834}\n",
      "Losses =  {'ner': 58643.876415252686}\n",
      "Losses =  {'ner': 60761.7693901062}\n",
      "Losses =  {'ner': 62557.531047821045}\n",
      "Losses =  {'ner': 64476.0532951355}\n",
      "Losses =  {'ner': 66121.19767379761}\n",
      "Losses =  {'ner': 68334.73698043823}\n",
      "Losses =  {'ner': 70209.5873374939}\n",
      "Losses =  {'ner': 71770.15089035034}\n",
      "Losses =  {'ner': 73873.05079269409}\n",
      "Losses =  {'ner': 74742.61805343628}\n",
      "Losses =  {'ner': 785.4195499420166}\n",
      "Losses =  {'ner': 1469.1436767578125}\n",
      "Losses =  {'ner': 2318.483673095703}\n",
      "Losses =  {'ner': 3927.5457611083984}\n",
      "Losses =  {'ner': 5913.588653564453}\n",
      "Losses =  {'ner': 7600.180694580078}\n",
      "Losses =  {'ner': 9606.278823852539}\n",
      "Losses =  {'ner': 11237.441589355469}\n",
      "Losses =  {'ner': 13549.408615112305}\n",
      "Losses =  {'ner': 15322.387115478516}\n",
      "Losses =  {'ner': 17057.272415161133}\n",
      "Losses =  {'ner': 18882.170837402344}\n",
      "Losses =  {'ner': 20289.94125366211}\n",
      "Losses =  {'ner': 21586.879943847656}\n",
      "Losses =  {'ner': 23187.136016845703}\n",
      "Losses =  {'ner': 24788.997436523438}\n",
      "Losses =  {'ner': 26659.603927612305}\n",
      "Losses =  {'ner': 28253.86604309082}\n",
      "Losses =  {'ner': 30089.56903076172}\n",
      "Losses =  {'ner': 31496.009048461914}\n",
      "Losses =  {'ner': 33293.31349182129}\n",
      "Losses =  {'ner': 35308.62649536133}\n",
      "Losses =  {'ner': 37191.52282714844}\n",
      "Losses =  {'ner': 39264.484313964844}\n",
      "Losses =  {'ner': 40866.8507232666}\n",
      "Losses =  {'ner': 42571.98583984375}\n",
      "Losses =  {'ner': 44458.87112426758}\n",
      "Losses =  {'ner': 46122.99430847168}\n",
      "Losses =  {'ner': 47785.89854431152}\n",
      "Losses =  {'ner': 49952.70587158203}\n",
      "Losses =  {'ner': 51627.9733581543}\n",
      "Losses =  {'ner': 53374.309005737305}\n",
      "Losses =  {'ner': 54881.89212036133}\n",
      "Losses =  {'ner': 56668.715270996094}\n",
      "Losses =  {'ner': 58582.53073120117}\n",
      "Losses =  {'ner': 60188.71716308594}\n",
      "Losses =  {'ner': 61673.82875061035}\n",
      "Losses =  {'ner': 63518.93943786621}\n",
      "Losses =  {'ner': 65369.03062438965}\n",
      "Losses =  {'ner': 67482.58638000488}\n",
      "Losses =  {'ner': 69296.68733215332}\n",
      "Losses =  {'ner': 70549.51905822754}\n",
      "Losses =  {'ner': 72385.48623657227}\n",
      "Losses =  {'ner': 73245.3521270752}\n",
      "Losses =  {'ner': 431.31174087524414}\n",
      "Losses =  {'ner': 938.9942016601562}\n",
      "Losses =  {'ner': 2269.990104675293}\n",
      "Losses =  {'ner': 3688.645896911621}\n",
      "Losses =  {'ner': 5436.721977233887}\n",
      "Losses =  {'ner': 7167.527534484863}\n",
      "Losses =  {'ner': 9475.399391174316}\n",
      "Losses =  {'ner': 11437.877220153809}\n",
      "Losses =  {'ner': 13445.476203918457}\n",
      "Losses =  {'ner': 14959.074729919434}\n",
      "Losses =  {'ner': 16634.919639587402}\n",
      "Losses =  {'ner': 18432.80962371826}\n",
      "Losses =  {'ner': 20208.661979675293}\n",
      "Losses =  {'ner': 21941.893714904785}\n",
      "Losses =  {'ner': 23400.44945526123}\n",
      "Losses =  {'ner': 25249.834953308105}\n",
      "Losses =  {'ner': 26995.524040222168}\n",
      "Losses =  {'ner': 28692.43190765381}\n",
      "Losses =  {'ner': 30556.378273010254}\n",
      "Losses =  {'ner': 32873.72165679932}\n",
      "Losses =  {'ner': 34206.343894958496}\n",
      "Losses =  {'ner': 35995.768058776855}\n",
      "Losses =  {'ner': 37560.47679901123}\n",
      "Losses =  {'ner': 39376.481925964355}\n",
      "Losses =  {'ner': 41347.12340545654}\n",
      "Losses =  {'ner': 42900.78832244873}\n",
      "Losses =  {'ner': 44430.72214508057}\n",
      "Losses =  {'ner': 46561.19730377197}\n",
      "Losses =  {'ner': 48302.249504089355}\n",
      "Losses =  {'ner': 49485.05004119873}\n",
      "Losses =  {'ner': 51118.33385467529}\n",
      "Losses =  {'ner': 52645.65813446045}\n",
      "Losses =  {'ner': 54502.75528717041}\n",
      "Losses =  {'ner': 56044.428886413574}\n",
      "Losses =  {'ner': 58068.35176849365}\n",
      "Losses =  {'ner': 59748.53325653076}\n",
      "Losses =  {'ner': 61073.38842010498}\n",
      "Losses =  {'ner': 62647.609214782715}\n",
      "Losses =  {'ner': 64531.38683319092}\n",
      "Losses =  {'ner': 65925.31801605225}\n",
      "Losses =  {'ner': 67848.12619781494}\n",
      "Losses =  {'ner': 69742.23667144775}\n",
      "Losses =  {'ner': 71457.40479278564}\n",
      "Losses =  {'ner': 72046.78769683838}\n",
      "Losses =  {'ner': 442.78849029541016}\n",
      "Losses =  {'ner': 964.2636947631836}\n",
      "Losses =  {'ner': 1849.748275756836}\n",
      "Losses =  {'ner': 3592.9525756835938}\n",
      "Losses =  {'ner': 5600.18017578125}\n",
      "Losses =  {'ner': 7059.903030395508}\n",
      "Losses =  {'ner': 8885.811782836914}\n",
      "Losses =  {'ner': 10790.606002807617}\n",
      "Losses =  {'ner': 12490.747177124023}\n",
      "Losses =  {'ner': 14088.092697143555}\n",
      "Losses =  {'ner': 15805.040161132812}\n",
      "Losses =  {'ner': 17482.885848999023}\n",
      "Losses =  {'ner': 19445.6388092041}\n",
      "Losses =  {'ner': 21348.254638671875}\n",
      "Losses =  {'ner': 23241.24591064453}\n",
      "Losses =  {'ner': 24631.16340637207}\n",
      "Losses =  {'ner': 26287.37550354004}\n",
      "Losses =  {'ner': 27820.41438293457}\n",
      "Losses =  {'ner': 29444.458602905273}\n",
      "Losses =  {'ner': 30941.224601745605}\n",
      "Losses =  {'ner': 32697.838340759277}\n",
      "Losses =  {'ner': 34477.10453033447}\n",
      "Losses =  {'ner': 36525.720542907715}\n",
      "Losses =  {'ner': 38051.72545623779}\n",
      "Losses =  {'ner': 39486.17095184326}\n",
      "Losses =  {'ner': 41032.67255401611}\n",
      "Losses =  {'ner': 42591.74990081787}\n",
      "Losses =  {'ner': 44416.54857635498}\n",
      "Losses =  {'ner': 46035.7144241333}\n",
      "Losses =  {'ner': 48119.12851715088}\n",
      "Losses =  {'ner': 49706.88318634033}\n",
      "Losses =  {'ner': 51655.56594085693}\n",
      "Losses =  {'ner': 53591.6823348999}\n",
      "Losses =  {'ner': 55635.702293395996}\n",
      "Losses =  {'ner': 57306.953941345215}\n",
      "Losses =  {'ner': 59035.21773529053}\n",
      "Losses =  {'ner': 60555.68613433838}\n",
      "Losses =  {'ner': 62263.135261535645}\n",
      "Losses =  {'ner': 64064.07297515869}\n",
      "Losses =  {'ner': 65554.88339996338}\n",
      "Losses =  {'ner': 66983.94255828857}\n",
      "Losses =  {'ner': 68726.25528717041}\n",
      "Losses =  {'ner': 70682.96523284912}\n",
      "Losses =  {'ner': 71757.84038543701}\n"
     ]
    }
   ],
   "source": [
    "epochs = 50\n",
    "\n",
    "with nlp.disable_pipes(*disable_pipes):\n",
    "  optimizer = nlp.begin_training()\n",
    " \n",
    "  for epoch in range(epochs):\n",
    "    st = time.time()\n",
    "    random.shuffle(train_data)\n",
    "    losses = {}\n",
    "\n",
    "    batches = minibatch(train_data, size=compounding(16.0, 64.0, 1.5))\n",
    "    for batch in batches:\n",
    "      text, annotation = zip(*batch)\n",
    "      nlp.update(text, annotation, drop=0.5, losses=losses, sgd=optimizer)\n",
    "      if epoch % 10 == 0:\n",
    "        print(\"Losses = \",losses)\n",
    "\n",
    "    epoch_time = time.time() - st\n",
    "    # print(\"Time taken - {} mins\",.format(epoch_time/60))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "3GdkBKwxGhjo",
    "outputId": "daf3f048-2b2a-433b-b4e1-f59c4ab5ad7f"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Torsade de pointes ventricular tachycardia during low dose intermittent dobutamine treatment in a patient with dilated cardiomyopathy and congestive heart failure .\n",
      "{'entities': [(0, 7, 'B_Dis'), (8, 10, 'I_Dis'), (11, 18, 'I_Dis'), (19, 30, 'B_Dis'), (31, 42, 'I_Dis'), (111, 118, 'B_Dis'), (119, 133, 'I_Dis'), (138, 148, 'B_Dis'), (149, 154, 'I_Dis'), (155, 162, 'I_Dis')]}\n"
     ]
    }
   ],
   "source": [
    "print(test_data[0][0])\n",
    "print(test_data[0][1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Testing newly trained model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 124
    },
    "id": "ApFq69QmNZDm",
    "outputId": "d3d37c85-a213-4992-9ddf-14192c348baa"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Entities [('pointes', 'B_Dis'), ('ventricular', 'I_Dis'), ('tachycardia', 'I_Dis'), ('dilated', 'B_Dis'), ('cardiomyopathy', 'I_Dis'), ('congestive', 'B_Dis'), ('heart', 'I_Dis'), ('failure', 'I_Dis')]\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<span class=\"tex2jax_ignore\"><div class=\"entities\" style=\"line-height: 2.5; direction: ltr\">Torsade de \n",
       "<mark class=\"entity\" style=\"background: #ddd; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    pointes\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; text-transform: uppercase; vertical-align: middle; margin-left: 0.5rem\">B_Dis</span>\n",
       "</mark>\n",
       " \n",
       "<mark class=\"entity\" style=\"background: #ddd; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    ventricular\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; text-transform: uppercase; vertical-align: middle; margin-left: 0.5rem\">I_Dis</span>\n",
       "</mark>\n",
       " \n",
       "<mark class=\"entity\" style=\"background: #ddd; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    tachycardia\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; text-transform: uppercase; vertical-align: middle; margin-left: 0.5rem\">I_Dis</span>\n",
       "</mark>\n",
       " during low dose intermittent dobutamine treatment in a patient with \n",
       "<mark class=\"entity\" style=\"background: #ddd; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    dilated\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; text-transform: uppercase; vertical-align: middle; margin-left: 0.5rem\">B_Dis</span>\n",
       "</mark>\n",
       " \n",
       "<mark class=\"entity\" style=\"background: #ddd; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    cardiomyopathy\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; text-transform: uppercase; vertical-align: middle; margin-left: 0.5rem\">I_Dis</span>\n",
       "</mark>\n",
       " and \n",
       "<mark class=\"entity\" style=\"background: #ddd; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    congestive\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; text-transform: uppercase; vertical-align: middle; margin-left: 0.5rem\">B_Dis</span>\n",
       "</mark>\n",
       " \n",
       "<mark class=\"entity\" style=\"background: #ddd; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    heart\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; text-transform: uppercase; vertical-align: middle; margin-left: 0.5rem\">I_Dis</span>\n",
       "</mark>\n",
       " \n",
       "<mark class=\"entity\" style=\"background: #ddd; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    failure\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; text-transform: uppercase; vertical-align: middle; margin-left: 0.5rem\">I_Dis</span>\n",
       "</mark>\n",
       " .</div></span>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "doc = nlp(test_data[0][0])\n",
    "print(\"Entities\", [(ent.text, ent.label_) for ent in doc.ents])\n",
    "displacy.render(doc, jupyter=True, style = \"ent\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 173
    },
    "id": "xyuPH7qyDyHG",
    "outputId": "2c8b7630-ceaf-40bc-c248-284e4136083d"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Entities [('visual', 'B_Dis'), ('attention', 'I_Dis'), ('deterioration', 'B_Dis')]\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<span class=\"tex2jax_ignore\"><div class=\"entities\" style=\"line-height: 2.5; direction: ltr\">A driving simulator ( Transport Research Laboratory ) was used to measure reaction time ( RT ) , speed maintenance and steering accuracy . Tests of basic visual function included high - and low - contrast visual acuity ( HCVA and LCVA ) , Pelli - Robson contrast threshold ( CT ) and Goldmann perimetry ( FIELDS ) . Useful Field of View ( UFOV - - a test of \n",
       "<mark class=\"entity\" style=\"background: #ddd; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    visual\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; text-transform: uppercase; vertical-align: middle; margin-left: 0.5rem\">B_Dis</span>\n",
       "</mark>\n",
       " \n",
       "<mark class=\"entity\" style=\"background: #ddd; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    attention\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; text-transform: uppercase; vertical-align: middle; margin-left: 0.5rem\">I_Dis</span>\n",
       "</mark>\n",
       " ) was also undertaken . The mean differences in the pre - and post - dilatation measurements were tested for statistical significance at the 95 % level using one - tail paired t - tests . RESULTS : Pupillary dilation resulted in a statistically significant \n",
       "<mark class=\"entity\" style=\"background: #ddd; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    deterioration\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; text-transform: uppercase; vertical-align: middle; margin-left: 0.5rem\">B_Dis</span>\n",
       "</mark>\n",
       " in CT and HCVA only .</div></span>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "doc = nlp(test_data[100][0])\n",
    "print(\"Entities\", [(ent.text, ent.label_) for ent in doc.ents])\n",
    "displacy.render(doc, jupyter=True, style = \"ent\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**We can see that the newly trained model was able to identify most of named-entities. So, our model is able to learn semantic relationships between the domain specific entities.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "id": "g6HhjKFqGjvH"
   },
   "outputs": [],
   "source": [
    "## Saving just NER model to disk\n",
    "output_dir = Path(\"/content/spacy_example\")\n",
    "nlp.to_disk(output_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 69
    },
    "id": "GE3tww07G2qx",
    "outputId": "c3aff985-c999-45d3-d85a-88314d948acf"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Entities [('CML', 'B_Dis')]\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<span class=\"tex2jax_ignore\"><div class=\"entities\" style=\"line-height: 2.5; direction: ltr\">\n",
       "<mark class=\"entity\" style=\"background: #ddd; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    CML\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; text-transform: uppercase; vertical-align: middle; margin-left: 0.5rem\">B_Dis</span>\n",
       "</mark>\n",
       " has a well - documented association with ionizing radiation , but reports of associations with chemical exposures have been questioned .</div></span>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "nlp_retrained = spacy.load(output_dir)\n",
    "doc = nlp_retrained(test_data[2000][0])\n",
    "print(\"Entities\", [(ent.text, ent.label_) for ent in doc.ents])\n",
    "displacy.render(doc, jupyter=True, style = \"ent\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 104
    },
    "id": "t-_ZK73tHldm",
    "outputId": "df024e4f-81d1-4e0c-fd6b-e8cde54f00f3"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Entities [('catalepsy', 'B_Dis')]\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<span class=\"tex2jax_ignore\"><div class=\"entities\" style=\"line-height: 2.5; direction: ltr\">The biochemical assays on the influence of four analgesics on the brain concentration and turnover of noradrenaline ( NA ) were also performed . It was found that three drugs stimulating central NA receptors failed to affect the analgesic ED50 of all antinociceptive agents and they enhanced \n",
       "<mark class=\"entity\" style=\"background: #ddd; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    catalepsy\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; text-transform: uppercase; vertical-align: middle; margin-left: 0.5rem\">B_Dis</span>\n",
       "</mark>\n",
       " induced by morphine and fentanyl .</div></span>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "doc = nlp_retrained(test_data[2200][0])\n",
    "print(\"Entities\", [(ent.text, ent.label_) for ent in doc.ents])\n",
    "displacy.render(doc, jupyter=True, style = \"ent\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "EPxMZLRlTFs8"
   },
   "source": [
    "## Problems with retraining NLP models :-\n",
    "\n",
    "Retraining NLP models with newly introduced data lead to Catastrophic Forgetting. In the context of NER, after retraining it may happen that the model fails to classify some named entity which it was able to classify before we newly trained it.\n",
    "\n",
    "A solution to this problem is **Pseudo-rehearsal**. Simply said we need to augment our data with the type of data that the model was originally trained with,i.e, we need to have some examples of the data that the model had originally seen when it was trained initially. Explained in details here - https://explosion.ai/blog/pseudo-rehearsal-catastrophic-forgetting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "8PjeRuEISNVF"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "Custom_NER_using_SpaCy.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
